{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from nmt_dataset.nmt_dataset import \\\n",
    "    NMTDataset, GO_token, EOS_token, SEQ_MXLEN\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set PyTorch device according to system offering\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of an item from the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [6],\n",
       "         [7],\n",
       "         [1],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2]]),\n",
       " tensor([[3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [6],\n",
       "         [7],\n",
       "         [1],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish \"global\" hyperparameter variables and load in the dataset\n",
    "HIDDEN_N = 128\n",
    "BATCH_SIZE = 1 # TODO: Needs to be increased\n",
    "DATASET_SIZE = BATCH_SIZE * (100000 // BATCH_SIZE)\n",
    "dataset = NMTDataset('nmt_dataset/eng-fra.txt', DATASET_SIZE)\n",
    "dataset_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print('Example of an item from the dataset:')\n",
    "dataset_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embedding_layer): Embedding(3987, 128)\n",
       "  (rnn_cell): GRU(128, 128, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an encoder model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_input: int, n_hidden: int) -> None:\n",
    "        super().__init__()\n",
    "        self.n_input, self.n_hidden = n_input, n_hidden\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(n_input, n_hidden)\n",
    "        self.rnn_cell = nn.GRU(n_hidden, n_hidden, batch_first=True)\n",
    "\n",
    "    def forward(self, _x, _hn):\n",
    "        _x_embedded = self.embedding_layer(_x).view(BATCH_SIZE, 1, -1)\n",
    "        return self.rnn_cell(_x_embedded, _hn)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.n_hidden, device=device)\n",
    "\n",
    "encoder = Encoder(dataset.input_lang.n_words, HIDDEN_N).to(device)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(2567, 128)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (attention): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (w_c): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (rnn_cell): GRU(128, 128, batch_first=True)\n",
       "  (w_y): Linear(in_features=128, out_features=2567, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a decoder model\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_hidden: int, n_output: int,\n",
    "        dropout_rate: float = 0.1\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_hidden, self.n_output = n_hidden, n_output\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_output, self.n_hidden)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.attention = nn.Linear(n_hidden, n_hidden)\n",
    "        self.w_c = nn.Linear(n_hidden * 2, n_hidden)\n",
    "        self.rnn_cell = nn.GRU(n_hidden, n_hidden, batch_first=True)\n",
    "        self.w_y = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, _x, _hn, _encoder_outputs):\n",
    "        _x_embedded = self.embedding(_x).view(BATCH_SIZE, 1, -1)\n",
    "        _x_embedded = self.dropout(_x_embedded)\n",
    "        _, _hn = self.rnn_cell(_x_embedded, _hn)\n",
    "        _alignment_scores = torch.mm(\n",
    "            self.attention(_hn)[0], _encoder_outputs.t())\n",
    "        _attention_weights = nn.functional.softmax(\n",
    "            _alignment_scores, dim=1)\n",
    "        _c_t = torch.mm(_attention_weights, _encoder_outputs)\n",
    "        _hidden_s_t = torch.cat([_hn[0], _c_t], dim=1)\n",
    "        _hidden_s_t = torch.tanh(self.w_c(_hidden_s_t))\n",
    "        _output = nn.functional.log_softmax(self.w_y(_hidden_s_t), dim=1)\n",
    "        return _output, _hn, _attention_weights\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.n_hidden, device=device)\n",
    "\n",
    "decoder = Decoder(HIDDEN_N, dataset.output_lang.n_words).to(device)\n",
    "decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function and optimizers for training the above models\n",
    "loss_function = nn.NLLLoss()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters())\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run a function to train the above models\n",
    "def train(\n",
    "    _encoder: Encoder, _decoder: Decoder, debug_step: int = None\n",
    ") -> tuple[Encoder, Decoder]:\n",
    "    total_loss = 0\n",
    "    for batch_index, (seq1, seq2) in enumerate(dataset_loader):\n",
    "        seq1 = seq1.to(device).squeeze(0) # TODO: get rid of squeeze for batch size > 1?\n",
    "        seq2 = seq2.to(device).squeeze(0)\n",
    "\n",
    "        # TODO: Sequences will need to be padded! (See pad token in nmt_dataset.py)\n",
    "\n",
    "        encoder_hn = _encoder.init_hidden()\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        len_seq1 = seq1.size(0) # TODO: This will need to be updated to be set to the current\n",
    "                                #       sequence length for the given example currently being used.\n",
    "        len_seq2 = seq2.size(0) # TODO: Same as above\n",
    "\n",
    "        encoder_outputs = torch.zeros(\n",
    "            SEQ_MXLEN, _encoder.n_hidden, device=device)\n",
    "\n",
    "        loss = torch.Tensor([0]).squeeze().to(device) # adjust loss vector have value for each sequence pair?\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            for encoder_index in range(len_seq1):\n",
    "                encoder_output, encoder_hn = _encoder(\n",
    "                    seq1[encoder_index], encoder_hn)\n",
    "                encoder_outputs[encoder_index] = encoder_output[0, 0]\n",
    "\n",
    "            decoder_input = torch.tensor([[GO_token]], device=device)\n",
    "            decoder_hn = encoder_hn\n",
    "\n",
    "            for decoder_index in range(len_seq2):\n",
    "                decoder_output, decoder_hn, _ = _decoder(\n",
    "                    decoder_input, decoder_hn, encoder_outputs\n",
    "                )\n",
    "                loss += loss_function(decoder_output, seq2[decoder_index])\n",
    "                decoder_input = seq2[decoder_index]\n",
    "\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() / len_seq2\n",
    "\n",
    "        if debug_step:\n",
    "            if batch_index % debug_step == 0:\n",
    "                sys.stdout.write(\n",
    "                    f'\\r{batch_index // debug_step:3d}/'\n",
    "                    f'{DATASET_SIZE // debug_step:3d} | '\n",
    "                    f'Loss: {total_loss / debug_step:3.2f}')\n",
    "                sys.stdout.flush()\n",
    "                total_loss = 0\n",
    "    return _encoder, _decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0/100 | Loss: 0.01"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(encoder, decoder, debug_step\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(_encoder, _decoder, debug_step)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     27\u001b[0m     \u001b[39mfor\u001b[39;00m encoder_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(len_seq1):\n\u001b[0;32m---> 28\u001b[0m         encoder_output, encoder_hn \u001b[39m=\u001b[39m _encoder(\n\u001b[1;32m     29\u001b[0m             seq1[encoder_index], encoder_hn)\n\u001b[1;32m     30\u001b[0m         encoder_outputs[encoder_index] \u001b[39m=\u001b[39m encoder_output[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m     decoder_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[GO_token]], device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1529\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1527\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1529\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1534\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1536\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1537\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1538\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1540\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1541\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, _x, _hn)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, _x, _hn):\n\u001b[1;32m     11\u001b[0m     _x_embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_layer(_x)\u001b[39m.\u001b[39mview(BATCH_SIZE, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn_cell(_x_embedded, _hn)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1529\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1527\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1529\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1534\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1536\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1537\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1538\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1540\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1541\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1128\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m   1127\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1128\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m   1129\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[1;32m   1130\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m   1132\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, debug_step=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12176a5713a61e315a544f914797e623fb5c9fee521b0a703da7d6cbf9676ed4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
