{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8\n",
    "\n",
    "In the module, the attention mechanism implementation is based on a single sequence input and iterated over every token individually. This implementation matches the block diagram in the module. \n",
    "\n",
    "Convert the implementation to batched using `BATCH_SIZE` to take advantage of PyTorch and a GPGPU device. Code updates and hints towards batch-by-batch processing have been provided throughout the implementation, including the dataset class NMTDataset. \n",
    "\n",
    "General steps towards batching in an RNN attention neural network:\n",
    "\n",
    "- Each sequence is created as a fixed-length tensor and padded to fill the tokens to the \n",
    "length after the EOS token. \n",
    "- Training has to input the sequences batch by batch.\n",
    "- It is permissible to go over the tokens one by one, but in batches.\n",
    "- RNN layers have batch_first to control the batch order, either batch-sequence-features or sequence-batch-features for True and False, respectively. The initial hidden \n",
    "layer order is always the same, sequence-batch-features\n",
    "\n",
    "Show that batched implementation is faster and generates the same error convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from nmt_dataset.nmt_dataset import \\\n",
    "    NMTDataset, GO_token, EOS_token, SEQ_MXLEN\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set PyTorch device according to system offering\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of an item from the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [1],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2]]),\n",
       " tensor([[3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [6],\n",
       "         [7],\n",
       "         [1],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish \"global\" hyperparameter variables and load in the dataset\n",
    "HIDDEN_N = 128\n",
    "BATCH_SIZE = 8\n",
    "DATASET_SIZE = BATCH_SIZE * (100000 // BATCH_SIZE)\n",
    "dataset = NMTDataset('nmt_dataset/eng-fra.txt', DATASET_SIZE)\n",
    "dataset_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print('Example of an item from the dataset:')\n",
    "dataset_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embedding_layer): Embedding(3989, 128)\n",
       "  (rnn_cell): GRU(128, 128, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an encoder model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_input: int, n_hidden: int) -> None:\n",
    "        super().__init__()\n",
    "        self.n_input, self.n_hidden = n_input, n_hidden\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(n_input, n_hidden)\n",
    "        self.rnn_cell = nn.GRU(n_hidden, n_hidden, batch_first=True)\n",
    "\n",
    "    def forward(self, _x, _hn):\n",
    "        _x_embedded = self.embedding_layer(_x).view(BATCH_SIZE, 1, -1)\n",
    "        return self.rnn_cell(_x_embedded, _hn)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, BATCH_SIZE, self.n_hidden, device=device)\n",
    "\n",
    "encoder = Encoder(dataset.input_lang.n_words, HIDDEN_N).to(device)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(2569, 128)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (attention): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (w_c): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (rnn_cell): GRU(128, 128, batch_first=True)\n",
       "  (w_y): Linear(in_features=128, out_features=2569, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a decoder model\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_hidden: int, n_output: int,\n",
    "        dropout_rate: float = 0.1\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_hidden, self.n_output = n_hidden, n_output\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_output, self.n_hidden)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.attention = nn.Linear(n_hidden, n_hidden)\n",
    "        self.w_c = nn.Linear(n_hidden * 2, n_hidden)\n",
    "        self.rnn_cell = nn.GRU(n_hidden, n_hidden, batch_first=True)\n",
    "        self.w_y = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, _x, _hn, _encoder_outputs):\n",
    "        _x_embedded = self.embedding(_x).view(BATCH_SIZE, 1, -1)\n",
    "        _x_embedded = self.dropout(_x_embedded)\n",
    "        _, _hn = self.rnn_cell(_x_embedded, _hn)\n",
    "        _alignment_scores = torch.bmm(\n",
    "            self.attention(_hn).permute(1, 0, 2),\n",
    "            _encoder_outputs.permute(0, 2, 1)\n",
    "        )\n",
    "        _attention_weights = nn.functional.softmax(\n",
    "            _alignment_scores, dim=2)\n",
    "        _c_t = torch.bmm(_attention_weights, _encoder_outputs)\n",
    "        _hidden_s_t = torch.cat([_hn.permute(1, 0, 2), _c_t], dim=2)\n",
    "        _hidden_s_t = torch.tanh(self.w_c(_hidden_s_t))\n",
    "        _output = nn.functional.log_softmax(self.w_y(_hidden_s_t), dim=2)\n",
    "        return _output, _hn, _attention_weights\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, BATCH_SIZE, self.n_hidden, device=device)\n",
    "\n",
    "decoder = Decoder(HIDDEN_N, dataset.output_lang.n_words).to(device)\n",
    "decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function and optimizers for training the above models\n",
    "loss_function = nn.NLLLoss()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters())\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run a function to train the above models\n",
    "def train(\n",
    "    _encoder: Encoder, _decoder: Decoder, debug_step: int = None\n",
    ") -> tuple[Encoder, Decoder]:\n",
    "    total_loss = 0\n",
    "    for batch_index, (seq1, seq2) in enumerate(dataset_loader):\n",
    "        seq1 = seq1.to(device)\n",
    "        seq2 = seq2.to(device)\n",
    "\n",
    "        encoder_hn = _encoder.init_hidden()\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs = torch.zeros(\n",
    "            SEQ_MXLEN, BATCH_SIZE, _encoder.n_hidden, device=device)\n",
    "\n",
    "        loss = torch.Tensor([0] * BATCH_SIZE).to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "\n",
    "            for encoder_index in range(SEQ_MXLEN):\n",
    "                encoder_output, encoder_hn = _encoder(\n",
    "                    seq1[:, encoder_index:encoder_index+1, :], encoder_hn)\n",
    "                encoder_outputs[encoder_index] = encoder_output.squeeze(1)\n",
    "            encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "            decoder_input = torch.tensor(\n",
    "                [[GO_token] * BATCH_SIZE], device=device)\n",
    "            decoder_hn = encoder_hn\n",
    "\n",
    "            for decoder_index in range(SEQ_MXLEN):\n",
    "                decoder_output, decoder_hn, _ = _decoder(\n",
    "                    decoder_input, decoder_hn, encoder_outputs)\n",
    "                loss += loss_function(\n",
    "                    decoder_output.squeeze(1),\n",
    "                    seq2[:, decoder_index, :].squeeze(1)\n",
    "                )\n",
    "                decoder_input = seq2[:, decoder_index, :].squeeze(1)\n",
    "\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() / SEQ_MXLEN\n",
    "\n",
    "        if debug_step:\n",
    "            if batch_index % debug_step == 0:\n",
    "                sys.stdout.write(\n",
    "                    f'\\r{batch_index // debug_step:3d}/'\n",
    "                    f'{DATASET_SIZE // debug_step:3d} | '\n",
    "                    f'Loss: {total_loss / debug_step:3.2f}')\n",
    "                sys.stdout.flush()\n",
    "                total_loss = 0\n",
    "    return _encoder, _decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Perform training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m train(encoder, decoder, debug_step\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[1;32m      4\u001b[0m runtime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRuntime (sec): \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mround\u001b[39m(runtime,\u001b[39m \u001b[39m\u001b[39m2\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(_encoder, _decoder, debug_step)\u001b[0m\n\u001b[1;32m     35\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_function(\n\u001b[1;32m     36\u001b[0m         decoder_output\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m),\n\u001b[1;32m     37\u001b[0m         seq2[:, decoder_index, :]\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     39\u001b[0m     decoder_input \u001b[39m=\u001b[39m seq2[:, decoder_index, :]\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     42\u001b[0m encoder_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     43\u001b[0m decoder_optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:524\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    515\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    516\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    517\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    523\u001b[0m     )\n\u001b[0;32m--> 524\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    525\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    526\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:260\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    251\u001b[0m inputs \u001b[39m=\u001b[39m (\n\u001b[1;32m    252\u001b[0m     (inputs,)\n\u001b[1;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, (torch\u001b[39m.\u001b[39mTensor, graph\u001b[39m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    257\u001b[0m )\n\u001b[1;32m    259\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> 260\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:133\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m    132\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    134\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m out\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mis_floating_point:\n\u001b[1;32m    137\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but got \u001b[39m\u001b[39m{\u001b[39;00mout\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "# Perform training\n",
    "start_time = time.time()\n",
    "train(encoder, decoder, debug_step=1000)\n",
    "runtime = time.time() - start_time\n",
    "print(f'Runtime (sec): {round(runtime, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for using the encoder\n",
    "# and decoder to translate a given sequence\n",
    "def translate_sequence(\n",
    "    _encoder: Encoder, _decoder: Decoder, _seq1,\n",
    "    max_length: int = SEQ_MXLEN\n",
    ") -> tuple:\n",
    "    with torch.no_grad():\n",
    "        SEQ_MXLEN = _seq1.size()[0]\n",
    "        encoder_hn = _encoder.init_hidden()\n",
    "        _seq1.to(device)\n",
    "\n",
    "        encoder_outputs = torch.zeros(\n",
    "            max_length, _encoder.n_hidden, device=device)\n",
    "        \n",
    "        for encoder_index in range(SEQ_MXLEN):\n",
    "            encoder_output, encoder_hn = _encoder(\n",
    "                _seq1[encoder_index], encoder_hn)\n",
    "            encoder_outputs[encoder_index] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[GO_token]], device=device)\n",
    "        decoder_hn = encoder_hn\n",
    "\n",
    "        decoder_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        for decoder_index in range(max_length):\n",
    "            decoder_output, decoder_hn, decoder_attention = _decoder(\n",
    "                decoder_input, decoder_hn, encoder_outputs)\n",
    "            decoder_attentions[decoder_index] = decoder_attention.data\n",
    "\n",
    "            # Output the word index with the highest probability\n",
    "            _, top_index = decoder_output.data.topk(1)\n",
    "            if top_index.item() != EOS_token:\n",
    "                decoder_words.append(\n",
    "                    dataset.output_lang.ix2word[top_index.item()])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            decoder_input = top_index.squeeze().detach()\n",
    "    \n",
    "    return decoder_words, decoder_attentions[:decoder_index+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for translating a given number of random sequences\n",
    "def translate_random_sequences(\n",
    "    _encoder: Encoder, _decoder: Decoder, n_seqs: int = 10\n",
    ") -> None:\n",
    "    for _ in range(n_seqs):\n",
    "        sample_index = random.randint(0, DATASET_SIZE - 1)\n",
    "        pair = dataset.pairs[sample_index]\n",
    "        seq1 = dataset[sample_index][0].to(device)\n",
    "\n",
    "        output_words, _ = translate_sequence(_encoder, _decoder, seq1)\n",
    "\n",
    "        print(f'seq1: {pair[0]} | seq2: {pair[1]} | '\n",
    "              f'result: {\" \".join(output_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the encoder/decoder model with a few random sequences\n",
    "translate_random_sequences(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to plot the attention values between a single pair\n",
    "def plot_attention(_seq1, _seq2, _attentions):\n",
    "    figure = plt.figure(figsize=(4, 4))\n",
    "    ax = figure.add_subplot(111)\n",
    "    cax = ax.matshow(_attentions, cmap='gray')\n",
    "    figure.colorbar(cax, fraction=0.03, pad=0.1)\n",
    "    _seq1 = _seq1.split(' ')\n",
    "    _seq2 = _seq2.split(' ')\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_xticks(np.arange(len(_seq1)))\n",
    "    ax.set_yticks(np.arange(len(_seq2)))\n",
    "    ax.set_xticklabels(_seq1, rotation=90)\n",
    "    ax.set_yticklabels(_seq2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to translate a sequence and plot the attention values\n",
    "def translate_and_plot_attention(\n",
    "    _encoder: Encoder, _decoder: Decoder, _seq1\n",
    ") -> None:\n",
    "    seq2, attentions = translate_sequence(\n",
    "        _encoder, _decoder, dataset.sentence_to_sequence(_seq1).to(device))\n",
    "    _seq1 += ' <EOS>'\n",
    "    seq2 = ' '.join(seq2) + ' <EOS>'\n",
    "    print(f'seq1: {_seq1} | seq2: {seq2}')\n",
    "    plot_attention(_seq1, seq2, attentions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate several sequences and plot their attentions\n",
    "translate_and_plot_attention(encoder, decoder, 'je ne compare pas tom a mary .')\n",
    "translate_and_plot_attention(encoder, decoder, 'elle est trop petit .')\n",
    "translate_and_plot_attention(encoder, decoder, 'je ne crains pas de mourir .')\n",
    "translate_and_plot_attention(encoder, decoder, 'tu es parfois si pueril .')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12176a5713a61e315a544f914797e623fb5c9fee521b0a703da7d6cbf9676ed4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
